var hostname = "https://meikle.io";
var index = lunr(function () {
    this.field('title')
    this.field('content', {boost: 10})
    this.field('category')
    this.field('tags')
    this.ref('id')
});



    index.add({
      title: "Development SSL for .NET Core and NGINX in Docker",
      category: ["opensource"],
      content: "\n    \n\nPhoto by James Sutton on Unsplash\n  \n\nWe‚Äôve all been there. You want to use SSL in development to mirror a production setup but it‚Äôs a pain to generate self-signed\ncertificates, share them with the development team, and have them trusted locally.\n\nThankfully combining Docker and the dotnet dev-certs command makes this nice and easy for .NET Core Applications and NGINX - \nwhich I‚Äôm sharing here so I don‚Äôt forget :)\n\n\n\n.NET Dev Certs Command\n\nTo make it easier for developers to generate and access development SSL certificates, the .NET Core provides\nthe dotnet dev-certs command as part of the CLI experience.\n\nThis command allows you to generate, export, trust and clean up self-signed developer certificates for your \napplications.\n\nThe beauty of this command on Window is that it adds the certificate to the trusted store locally meaning\nthat browsers will validate it as such.\n\nBy using this approach each developer can quickly generate their own certificates locally and mount them in Docker\nas part of a Docker Compose file.\n\n.NET Core Application\n\n.NET Core applications expect certificates in PFX format, so to generate a certificate in this format you can\nuse the ‚Äìformat pfx option for the  dotnet dev-certs command.\n\nBelow is an example of generating a certificate into a config folder:\n\ndotnet dev-certs https -ep ./config/app-certificate.pfx -p BlahBlah --trust --format pfx\n\n\nThis command will then create a new certificate and trust it in the machines certificate store, before exporting it\nto the PFX file, with the private key protected by the password BlahBlah.\n\nHaving done this, we can then mount this certificate as a volume in Docker and set some environment variables\nto configure the .NET Core application to use this certificate:\n\napp:\n  build:\n    context: app/.\n    dockerfile: Dockerfile\n  ports:\n    - \"50000:50000\"\n  environment:\n    - ASPNETCORE_ENVIRONMENT=Development\n    - ASPNETCORE_URLS=https://+:50000\n    - ASPNETCORE_Kestrel__Certificates__Default__Password=BlahBlah\n    - ASPNETCORE_Kestrel__Certificates__Default__Path=/certs/app-certificate.pfx\n  volumes:\n    - ./config/app-certificate.pfx:/certs/app-certificate.pfx\n\n\nVoila! We have our .NET Core application listening on Port 50000 secured using SSL.\n\nNGINX Docker Image\n\nIn many cases your application will be running on HTTP with a reverse proxy in front of it, such\nas NGINX.\n\nTo use a similar approach with the NGINX docker image, you can use the ‚Äìformat pem option \nfor the  dotnet dev-certs command to generate a key pair (i.e. crt and key files) in the PEM format.\n\nBelow is an example of generating a certificate pair into a config folder:\ndotnet dev-certs https -ep ./config/certificate.crt -np --trust --format pem\n\n\nIn this example, the -np flag has been used to export the private key in the PEM file without the need\nfor a password to be provided to read it.\n\nIn this approach, the NGINX config file can be updated to used these keys in it‚Äôs appropriate server block:\n\nlisten              443 ssl;\nserver_name localhost;\nssl_certificate     /etc/nginx/proxy-certificate.crt;\nssl_certificate_key /etc/nginx/proxy-certificate.key;\n\n\nIf a password is desired, it can be provided in the export command:\ndotnet dev-certs https -ep ./config/certificate.crt -p BlahBlah --trust --format pem\n\n\nDoing this then requires you to pass a ssl_password_file to NGINX using the ssl_password_file directive. For example:\n\nlisten              443 ssl;\nserver_name localhost;\nssl_certificate     /etc/nginx/proxy-certificate.crt;\nssl_certificate_key /etc/nginx/proxy-certificate.key;\nssl_password_file /etc/nginx/proxy-certificate.pass;\n\n\nThis file is just a simple text file with the password for the key inside it, or a list of passphrases to be tried, if you\nwish to have one single password file for multiple certificates.\n\nJust like in the .NET Core Application example, these key files can be mounted as volumes:\n\nproxy:\n    image: nginx\n    ports:\n     - \"80:80\"\n     - \"443:443\"\n    volumes:\n      - ./config/nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./config/proxy-certificate.crt:/etc/nginx/proxy-certificate.crt\n      - ./config/proxy-certificate.key:/etc/nginx/proxy-certificate.key\n    depends_on:\n      - web-app\n    links:\n      - web-app\n\n\nRefreshing Certificates\n\nIf you want to start from scratch with a set of certificates, you can run the following command:\ndotnet dev-certs https --clean\n\nThis clears down the https certificates.\n\n",
      tags: ["dotnetcore","nginx","docker"],
      id: 0
    });
    

    index.add({
      title: "Okapi Maven Plugin",
      category: ["localization","opensource"],
      content: "\n    \n\nPhoto by Safar Safarov on Unsplash\n\nIn the work we did at Lingo24 when I was there, we made use of the Okapi Framework - well a customised fork - as a key component in our content processing.\n\nFor those who don‚Äôt know the framework, it an excellent swiss-army like toolkit for localisation (l18n)/translation (t8n), which can be used either as a set of libraries or standalone tools.\n\nWhilst the tools available are great, often I wanted to bundle part of the process within an existing CI/CD pipeline or build. As a heavy Maven user, and with winter nights setting in, the idea came to build a Maven plugin to do just that.\n\nThe result? The Okapi Maven Plugin.\n\n\n\nThe Okapi Maven Plugin\n\nThe Okapi Maven Plugin is a Maven plugin that allows execution of common actions using the Okapi Framework within a Maven Project.\n\nIt was inspired by the excellent okapi-ant project, written by Chase Tingley, one of the core Okapi Framework team. (Hey Chase üëãüèª)\n\nVery much designed to scratch what was an immediate itch, it currently supports performing the following actions:\n\n  Execute a Pipeline\n  Export a Batch Configuration (BCONF)\n  Install a Batch Configuration (BCONF)\n\n\nUsing the Okapi Maven Plugin\nThe plugin is being released on Maven Central, so to use the plug-in you can simply include it in your project‚Äôs POM file in the plugins section.\n\n&lt;build&gt;\n  &lt;plugins&gt;\n    &lt;plugin&gt;\n      &lt;groupId&gt;io.meikle.maven.okapi&lt;/groupId&gt;\n      &lt;artifactId&gt;okapi-maven-plugin&lt;/artifactId&gt;\n      &lt;version&gt;1.0&lt;/version&gt;\n      &lt;configuration&gt;\n          &lt;!-- Configuration required here --&gt;\n      &lt;/configuration&gt;\n    &lt;/plugin&gt;\n  &lt;/plugins&gt;\n&lt;/build&gt;\n\n\nOkapi Version\n\nBy default, the plugin uses the latest Okapi Framework version, which at the time of writing this post is 1.43.0, with 1.44.0 coming soon.\n\nYou can validate the version being used by running the okapi:version goal.\nmvn okapi:version\n\n\nThis can be changed by overriding the Okapi Framework‚Äôs okapi-core and okapi-application-rainbow dependencies in the plugin configuration.\n\n&lt;build&gt;\n    &lt;plugins&gt;\n        &lt;plugin&gt;\n            &lt;groupId&gt;io.meikle.maven.okapi&lt;/groupId&gt;\n            &lt;artifactId&gt;okapi-maven-plugin&lt;/artifactId&gt;\n            &lt;version&gt;1.0&lt;/version&gt;\n            &lt;configuration&gt;\n                ...\n            &lt;/configuration&gt;\n            &lt;dependencies&gt;\n                &lt;dependency&gt;\n                    &lt;groupId&gt;net.sf.okapi&lt;/groupId&gt;\n                    &lt;artifactId&gt;okapi-core&lt;/artifactId&gt;\n                    &lt;version&gt;1.41.0&lt;/version&gt;\n                &lt;/dependency&gt;\n                &lt;dependency&gt;\n                    &lt;groupId&gt;net.sf.okapi.applications&lt;/groupId&gt;\n                    &lt;artifactId&gt;okapi-application-rainbow&lt;/artifactId&gt;\n                    &lt;version&gt;1.41.0&lt;/version&gt;\n                    &lt;!-- Exclude non-required UI dependencies --&gt;\n                    &lt;exclusions&gt;\n                        &lt;exclusion&gt;\n                            &lt;artifactId&gt;*&lt;/artifactId&gt;\n                            &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;\n                        &lt;/exclusion&gt;\n                    &lt;/exclusions&gt;\n                &lt;/dependency&gt;\n            &lt;/dependencies&gt;\n        &lt;/plugin&gt;\n    &lt;/plugins&gt;\n&lt;/build&gt;\n\n\nBinary Configurations\n\nOne of the main use cases for the plugin is to support the building of Binary Configurations, also known as a BCONF.\n\nA BCONF is a single file binary bundle of a number of configuration items:\n\n\n  File Filter Settings - the settings on how a file is processed\n  File Mappings - the mapping of file extensions to file filter settings\n  Pipelines - the configurations determing the steps to be performed on the content\n  Plugin JARs - optional JAR(s) with custom code, typically file filters or steps.\n\n\nThe ability to bundle them together in a single BCONF them useful for sharing settings amongst colleagues, or when using the Longhorn server application for executing Okapi Framework processing.\n\nBuilding BCONF files can be done from the Rainbow application interface provided by the Okapi Framework. To do this you configure the settings you want in the application and can then export a configuration.\n\nThe inverse process is also supported, allowing you to import a configuration to a particular directory so it‚Äôs contents can be used within the application.\n\nTo help with these operations, the Okapi Maven Plugin allows you to build and install BCONF files as part of your build.\n\nokapi:export\n\nThe export goal builds and export a Binary Configuration file and can be configured to package up configuration items in a number of scenarios.\n\nBy default, the goal only has one mandatory parameter, bconf. This is the name or path to where the Binary Configuration file should be exported to.\n\nThe rest of the parameters are optional, allowing you to either package a pipeline and the filter mappings and/or plugins required to successfully execute it within Longhorn, or simply bundle a set configuration assets for sharing.\n\n&lt;configuration&gt;\n    &lt;!-- Mandatory Name or Path to Export the BCONF to --&gt;\n    &lt;bconf&gt;export.bconf&lt;/bconf&gt;\n    &lt;!-- Optional Pipeline to be used in the BCONF --&gt;\n    &lt;pipeline&gt;test.pln&lt;/pipeline&gt;\n    &lt;!-- Optional Custom Filter Mapping(s) to be embedded in the BCONF --&gt;\n    &lt;filterMappings&gt; \n        &lt;filterMapping&gt;\n            &lt;!-- File extension to be mapped --&gt;\n            &lt;extension&gt;.htm&lt;/extension&gt;\n            &lt;!-- Configuration to be used --&gt;\n            &lt;configuration&gt;okf_html@test&lt;/configuration&gt;\n        &lt;/filterMapping&gt;\n    &lt;/filterMappings&gt;\n    &lt;!-- Optional Plugins to be included in the BCONF using Maven FileSet format- --&gt;\n    &lt;!-- See https://maven.apache.org/shared/file-management/fileset.html for more details --&gt;\n    &lt;plugins&gt;\n        &lt;!-- Optional directory - can be absolute (include Maven variables), or relative to project baseDir --&gt;\n        &lt;!-- defaults to project baseDir --&gt;\n        &lt;directory&gt;input&lt;/directory&gt;\n        &lt;!-- Optional - Includes or Excludes --&gt;\n        &lt;includes&gt;\n            &lt;include&gt;**/*.jar&lt;/include&gt;\n        &lt;/includes&gt;\n    &lt;/plugins&gt;\n    &lt;!-- Optional output directory for the Okapi pipeline --&gt;\n    &lt;!-- Defaults to the Maven target directory --&gt;\n    &lt;outputDirectory&gt;/tmp&lt;/outputDirectory&gt;\n&lt;/configuration&gt;\n\n\nokapi:install\n\nThe install goal installs a Binary Configuration file (a bconf) to a folder on the local machine so it can be used.\n\nThe configuration for this goal has two mandatory items:\n\n  bconf - the location of the BCONF file to install\n  outputDirectory - the folder to install the BCONF to\n\n\n&lt;configuration&gt;\n  &lt;bconf&gt;export.bconf&lt;/bconf&gt;\n  &lt;outputDirectory&gt;/tmp/install-test&lt;/outputDirectory&gt;\n&lt;/configuration&gt;\n\n\nBoth paths can be absolute or relative to the Maven Project‚Äôs baseDir, which can be useful when chaining with the pipeline goal.\n\nPipelines\n\nAt the core of the Okapi Framework is the concept of a pipeline - a list of actions in sequence that can be applied to or more input documents. Each action is a step in Okapi parlance, which when chained together can build very powerful and complex translation workflows.\n\nPipelines can be developed using the user interface in Okapi Rainbow or via an XML format, with the framework supplying a significant library of out-of-the-box steps and pipelines for common actions in translation workflow. Like everything in Okapi, you can build your own steps.\n\nTo help with executing pipelines, the Okapi Maven Plugin allows you to execute pre-defined pipelines as part of your build.\n\nokapi:execute\n\nThe execute goal executes a pipeline contained in a pipeline configuration file (a pln file) against the supplied input file(s).\n\nBy default, the goal has four mandatory parameters:\n\n  pipeline - the path to the pipeline file\n  inputFiles - the input files to process, defined using a FileSet\n  sourceLang - the source language name or code of the content\n  targetLang - the target language  name or code of the content\n\n\nThere are many other configuration options available to allow you to specify additional options for customised filter configurations, plugin locations, or override default locations. These can be useful if you are embedding your own custom assets into a build.\n\n\n&lt;configuration&gt;\n    &lt;!-- Mandatory Path to pipeline to execute --&gt;\n    &lt;pipeline&gt;test.pln&lt;/pipeline&gt;\n    &lt;!-- Mandatory inputFiles using Maven FileSet format--&gt;\n    &lt;!-- See https://maven.apache.org/shared/file-management/fileset.html for more details --&gt;\n    &lt;inputFiles&gt;\n        &lt;!-- Optional directory - can be absolute (include Maven variables), or relative to project baseDir --&gt;\n        &lt;!-- defaults to project baseDir --&gt;\n        &lt;directory&gt;input&lt;/directory&gt;\n        &lt;!-- Optional - Includes or Excludes --&gt;\n        &lt;includes&gt;\n            &lt;include&gt;**/*.txt&lt;/include&gt;\n            &lt;include&gt;**/*.md&lt;/include&gt;\n        &lt;/includes&gt;\n        &lt;excludes&gt;\n            &lt;exclude&gt;**/*.xml&lt;/exclude&gt;\n        &lt;/excludes&gt;\n    &lt;/inputFiles&gt;\n    &lt;!-- Mandatory Source Language Name or Code--&gt;\n    &lt;sourceLang&gt;English&lt;/sourceLang&gt;\n    &lt;!-- Mandatory Target Language Name or Code--&gt;\n    &lt;targetLang&gt;fr-FR&lt;/targetLang&gt;\n    &lt;!-- Optional directory containing custom plugins --&gt;\n    &lt;!-- Defaults to the project directory --&gt;\n    &lt;pluginsDirectory&gt;pluginsDir&lt;/pluginsDirectory&gt;\n    &lt;!-- Optional Explicit Plugins using Maven FileSet format- --&gt;\n    &lt;!-- See https://maven.apache.org/shared/file-management/fileset.html for more details --&gt;\n    &lt;plugins&gt;\n        &lt;!-- Optional directory - can be absolute (include Maven variables), or relative to project baseDir --&gt;\n        &lt;!-- defaults to project baseDir --&gt;\n        &lt;directory&gt;input&lt;/directory&gt;\n        &lt;!-- Optional - Includes or Excludes --&gt;\n        &lt;includes&gt;\n            &lt;include&gt;**/*.jar&lt;/include&gt;\n        &lt;/includes&gt;\n    &lt;/plugins&gt;\n    &lt;!-- Optional directory containing custom filters --&gt;\n    &lt;!-- Defaults to the project directory --&gt;\n    &lt;filtersDirectory&gt;filtersDir&lt;/filtersDirectory&gt;\n    &lt;!-- Optional output directory for the Okapi pipeline --&gt;\n    &lt;!-- Defaults to the Maven target directory --&gt;\n    &lt;outputDirectory&gt;/tmp&lt;/outputDirectory&gt;\n&lt;/configuration&gt;\n\n\nNext Steps\n\nWhilst I moved on from Lingo24 before being able to use the plugin in anger, I have continued to use it for automating actions on some personal projects. I plan to keep evolving it as and when I get a chance, so would love feedback on what could be better or additional features that would be handy.\n\nHappy translating!\n",
      tags: ["okapi","maven-plugin"],
      id: 1
    });
    

    index.add({
      title: "Apache Tika Docker Examples",
      category: ["technology","opensource"],
      content: "\n    \n\n\n    Photo by Priscilla Du Preez on Unsplash\n\n\nFor a number of years I‚Äôve been involved in the Apache Tika project as both a committer and PMC member.\n\nWith the increase in container technology usage over the past few years we spun up a separate repository for Apache Tika Server in Docker, called tika-docker with convenience images hosted on Docker Hub\n\nThis has resulted in questions on how to customise configuration and host instances that link to other services. To help people get started, we‚Äôve created some example scenarios.\n\nSo let‚Äôs dive in and check them out.\n\n\n\nThe tika-docker examples\n\nTo get the examples started, we‚Äôve created examples using Docker Compose of the following scenarios:\n\n\n  Recognising and Captioning Video and Images with TensorFlow REST (see here)\n  Enriching Academic PDF Parsing with Grobid REST (see here)\n  OCR of PDF or Images with Tesseract including a Custom  Configuration (see here)\n  Named Entity Recognition (see here)\n\n\nUsing the examples\n\nInstall Docker and Docker Compose\n\nFollow the instructions for install docker from here.\n\nFollow the instructions for installing docker-compose from here.\n\nClone the tika-docker\n\nNow fetch the docker-compose files and sample configuation from the tika-docker project on GitHub:\n\ngit clone https://github.com/apache/tika-docker\n\nRun Docker Compose for Example You Want\n\nFirst change into the tika-docker directory\n\ncd tika-docker\n\nThen you can execute docker-compose for the example you wish to try. For example, to try the Named Entity Recognition (NER) example you can:\n\ndocker-compose -f docker-compose-tika-ner.yml up -d\n\nYou can drop the -d if you want to stay attached to the containers.\n\nThen if you supplied a text file with some sample data to the /meta endpoint:\n\ncat &lt;&lt;EOT &gt;&gt; test.txt\nHello world from the Apache Tika Team (dev@tika.apache.org).\nEOT\ncurl -T test.txt http://localhost:9998/meta\n\nThe RegEx Entity Recogniser configured in the NER sample configuration files would extract the email in the returned metadata:\n\n\"X-Parsed-By\",\"org.apache.tika.parser.CompositeParser\",\"org.apache.tika.parser.ner.NamedEntityParser\"\n\"language\",\"en\"\n\"NER_EMAIL\",\"dev@tika.apache.org\"\n\"Content-Type\",\"text/plain\"\n\nYou can then stop the running containers using\n\ndocker-compose -f docker-compose-tika-ner.yml down\n\nCustomising the examples\n\nEach of the examples comes with associated set of configuration files in the sample-configs directory.\n\nEach sample has an appropriately named subfolder, with the associated Tika Config XML file(s) and any other configuration resources, such as properties files specifying URLs or settings.\n\n‚îú‚îÄ‚îÄ customocr\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ org\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ apache\n‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ tika\n‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ parser\n‚îÇ¬†¬† ‚îÇ¬†¬†             ‚îî‚îÄ‚îÄ ocr\n‚îÇ¬†¬† ‚îÇ¬†¬†                 ‚îî‚îÄ‚îÄ TesseractOCRConfig.properties\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ tika-config-inline.xml\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tika-config-rendered.xml\n‚îú‚îÄ‚îÄ grobid\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ org\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ apache\n‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ tika\n‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ parser\n‚îÇ¬†¬† ‚îÇ¬†¬†             ‚îî‚îÄ‚îÄ journal\n‚îÇ¬†¬† ‚îÇ¬†¬†                 ‚îî‚îÄ‚îÄ GrobidExtractor.properties\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tika-config.xml\n‚îú‚îÄ‚îÄ ner\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ run_tika_server.sh\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tika-config.xml\n‚îî‚îÄ‚îÄ vision\n    ‚îú‚îÄ‚îÄ inception-rest-caption.xml\n    ‚îú‚îÄ‚îÄ inception-rest-video.xml\n    ‚îî‚îÄ‚îÄ inception-rest.xml\n\nTika Config XML\n\nAll of the scenarios have Tika Config XML files. These files configure the parsers or recognisers for the example.\n\nIn some cases this file is named tika-config.xml and is then loaded in the docker-compose file directly. In other examples, such as the Vision and OCR ones, the docker-compose file loads only one of the XML configurations as the default tika-config.xml through a volume mount.\n\nFor example, in the the Vision configuration for the Parsing and Captioning Video or Images with TensorFlow REST, you have a choice of three configurations:\n\n\n  inception-rest-caption.xml - for image captioning\n  inception-rest-video.xml - for object recognition in videos\n  inception-rest.xml - for object recognition in images\n\n\nYou can chose which you want to use by leaving commented the appropriate configuration in the docker-compose-tika-vision.yml file within the volumes section:\n\n#... snip ...\n\nvolumes:\n  # Replace the below with the configuration you want to use, or with your own custom one \n  # -  ./sample-configs/vision/inception-rest.xml:/tika-config.xml\n  # -  ./sample-configs/vision/inception-rest-video.xml:/tika-config.xml\n  -  ./sample-configs/vision/inception-rest-caption.xml:/tika-config.xml\n\n#... snip ...\n\nYou can find more on ObjectRecognition and the TensorFlow in Apache Tika Server from my previous blog post here.\n\nWant more?\n\nI plan to write most specific blog posts, similar to the Tensorflow REST one, on each of these example scenarios. So please subscribe to the RSS feed for this blog if you are interested.\n\nIf you would like to see other examples like this, either let me know directly on GitHub or Twitter, or message on the Apache Tika Users or Developer mailing lists.\n",
      tags: ["apache","tika","examples"],
      id: 2
    });
    

    index.add({
      title: "Apache Tika and the ObjectRecognitionParser for Object Recognition and Captioning Using TensorFlow REST.",
      category: ["technology","opensource"],
      content: "\n    \n\nPhoto by John Schnobrich on Unsplash\n\nOne of the coolest new features added to Apache Tika in the past few years has been the addition of Parsers that leverage Deep Learning to perform object recognition and captioning.\n\nContributed by Chris Mattmann and Thejan Wijesinghe, through their work with USC Data Science, you can configure Apache Tika to call of to predefined models and get deep learning equivalent of ‚ÄòHello World‚Äô - tagging dog or cat pictures!\n\nSo let‚Äôs try it out.\n\n\n\nApache Tika and the ObjectRecognitionParser\n\nWhat is the ObjectRecognitionParser?\n\nThe ObjectRecognitionParser is a Parser that can be configured to recognise objects within content and annotate the metadata with information on the objects it has recognised.\n\nInternally, the recognised objects are returned in a RecognisedObject for generic objects or it‚Äôs CaptionObject sub-class for captioning:\n\n\n  RecognisedObject instances contain an ID, Label, Label Language and Confidence Score.\n  CaptionObject instances contain an ID, Caption Sentence, Caption Language and Confidence Score.\n\n\nBoth types are placed in the metadata collection during parsing.\n\nThe type of recognition to be performed needs to be defined within Apache Tika‚Äôs tika-config.xml through the configuration of ObjectRecogniser instances to be used by the parser.\n\nAvailable Object Recognisers\n\nThere are a number of ObjectRecogniser implementations in Apache Tika, including offline recognisers that need Deep Learning tools installed on the local machine (e.g. DL4J or Tensorflow) as well as online recognisers that make REST call to services.\n\nFor now we are going to focus on the online recognisers, specifically ones that use Tensorflow REST APIs runnable in Docker from the USC Data Science team.\n\nThese are:\n\n\n  TensorflowRESTRecogniser - which uses a custom REST API around Tensorflow to perform recognition on images\n  TensorflowRESTVideoRecogniser - which uses a custom REST API around Tensorflow to perform recognition on videos\n  TensorflowRESTCaptioner - which uses a custom REST API around Tensorflow to perform image captioning\n\n\nThese instances us an implementation based on the paper ‚ÄúShow and Tell: A Neural Image Caption Generator‚Äù for captioning images, and the Inception-V4 model from Tensorflow for recognition in video and images.\n\nI‚Äôll come back to the offline ones in another post.\n\nLet‚Äôs try it out on Tika Server\n\nGet the Docker Images\n\nTo make it easier to get up and running we‚Äôll use Apache Tika Docker and the helper docker-compose file.\n\nFirst, get the tika-docker project from GitHub:\n\ngit clone https://github.com/apache/tika-docker\n\nIn here is a file called docker-compose-tika-vision.yml which contains everything you need.\n\nTo make it easier we‚Äôll create a symlink to allow us to execute docker-compose without specifying the file each time:\n\nln -s docker-compose-tika-vision.yml docker-compose.yml\n\nConfigure our instance\n\nLike most things is Apache Tika, the ObjectRecogniser can be configured using the tika-config.xml file format.\n\nTo make things easier, there are three sample configuration to choose from to get your started:\n\n\n  sample-configs/inception-rest.xml - for image recognition\n  sample-configs/inception-rest-caption.xml - for image captioning\n  sample-configs/inception-rest-video.xml - for video recognition\n\n\nYou can do this by leaving only the configuration file entry you wish to use uncommented (or present) in the volumes section of the docker-compose file.\n\nFor example, to use image captioning you can leave the following set:\n\nvolumes:\n - ./sample-configs/inception-rest-caption.xml:/tika-config.xml\n\nRun Tika Server + Inception Services\n\nWith the above configuration set in the docker-compose.yml file, you can now load up the containers:\n\ndocker-compose up\n\nApache Tika Server will keep trying to reload until it can detect the configured Inception Service instance. If so want to avoid this, you can start the Inception Service first and then Tika.\n\nOnce they are loaded, you can now send some files to it:\n\nwget https://upload.wikimedia.org/wikipedia/commons/f/f6/Working_Dogs%2C_Handlers_Share_Special_Bond_DVIDS124942.jpg -O test.jpg\ncurl -T test.jpg htp://localhost:9998/meta\n\nThis should then give you suggested captions as part of the metadata collection parsed:\n\n\"org.apache.tika.parser.recognition.object.rec.impl\",\"org.apache.tika.parser.captioning.tf.TensorflowRESTCaptioner\"\n\"X-Parsed-By\",\"org.apache.tika.parser.CompositeParser\",\"org.apache.tika.parser.recognition.ObjectRecognitionParser\"\n\"language\",\"en\"\n\"CAPTION\",\"a man standing next to a dog on a leash . (0.00022)\",\"a man standing next to a dog on a bench . (0.00017)\",\"a man and a dog sitting on a bench . (0.00011)\",\"a man standing next to a dog in a park . (0.00007)\",\"a man and a dog sitting on a bench (0.00006)\"\n\"Content-Type\",\"image/jpeg\"\n\nHow about the Tika App?\nYou can also re-use the Inception Services from the docker-compose.yml file for the Apache Tika app interactively.\n\nTo do the captioning, you can just start the inception service you want - in this case inception-caption:\n\ndocker-compose up inception-caption \n\nYou can then create a custom tika-config.xml and setting the appropriate apiBaseUri\n\nEOT &gt;&gt; tika-config.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;properties&gt;\n    &lt;parsers&gt;\n        &lt;parser class=\"org.apache.tika.parser.recognition.ObjectRecognitionParser\"&gt;\n            &lt;mime&gt;image/jpeg&lt;/mime&gt;\n            &lt;mime&gt;image/png&lt;/mime&gt;\n            &lt;mime&gt;image/gif&lt;/mime&gt;\n            &lt;params&gt;\n                &lt;param name=\"apiBaseUri\" type=\"uri\"&gt;http://localhost:8765/inception/v3&lt;/param&gt;\n                &lt;param name=\"captions\" type=\"int\"&gt;5&lt;/param&gt;\n                &lt;param name=\"maxCaptionLength\" type=\"int\"&gt;15&lt;/param&gt;\n                &lt;param name=\"class\" type=\"string\"&gt;org.apache.tika.parser.captioning.tf.TensorflowRESTCaptioner&lt;/param&gt;\n            &lt;/params&gt;\n        &lt;/parser&gt;\n    &lt;/parsers&gt;\n&lt;/properties&gt;\nEOT\n\nIt‚Äôs worth noting in the sample configuration the apiBaseUri uses the Docker Compose service name and internal port. For running outside, you‚Äôll need to use the external facing port mapping and IP/Hostname of the machine it is running on.\n\nThen you can run the Apache Tika App JAR using your custom configuration. For example, to launch it in GUI mode you could use:\n\njava -jar tika-app-1.25.jar --config=tika-config.xml -g\n\nWhat‚Äôs next?\nThese REST based Tensorflow models are great examples of how Deep Learning can be used to augment the logic approach of Apache Tika for content parsing or detection.\n\nIf you want to try adding basic tagging or captioning to your search or asset pipelines, these models could provide a start, or the REST API implementation provide inspiration for hosting your own Tensorflow models.\n\nIt is an area that will continue to expand in the project and provides another API extension point where you can build your own ObjectRecogniser implementations. Happy Parsing!\n",
      tags: ["apache","tika","tensorflow"],
      id: 3
    });
    

    index.add({
      title: "HP Probook Touchpad Slow After Supend Ubuntu 19.04",
      category: ["technology"],
      content: "\n    \n    Photo of a Laptop Touchpad\n\n\nHaving got used to my replacement laptop, I‚Äôve decided to keep using it, but there was one thing annoying me - the trackpad!\n\nThe trackpad is definately not the same quality as the MacBook Pro I was used to, but I could cope as the main features are there. However, I often move around a lot going from meeting to meeting, making use of the sleep/suspend capability.\n\nThat was when the trouble started‚Ä¶\n\n\n\nSluggish Touch\n\nUpon waking from suspend, the touchpad was sluggish and jumping all over the place, rendering the machine unusable without a mouse!\n\nThe best way it seemed to get the touchpad to behave was to add post suspend script using systemd to reload the driver. A little hacky but effective!\n\nSystemd and Suspend/Hibernation Hooks\n\nIt is really easy to perform an action either before or after you suspend, with systemd providing a nice script based hook within its systemd-suspend.service system service.\n\nTo use this, all you need to do is put an executable script under /usr/lib/systemd/system-sleep/ that checks whether the first argument is ‚Äòpre‚Äô for before the system suspends, or ‚Äòpost‚Äô for after the system wakes. The script can have any name.\n\nYou can see more information in it‚Äôs man page:\n\nman systemd-suspend.service\n\nThe Script\n\nTo sort this once and for all on the HP ProBook I wrote the following into a file called /lib/systemd/system-sleep/touchpad-reload:\n\n#!/bin/sh\ncase $1 in\n  post)\n    /sbin/rmmod i2c_hid &amp;&amp; /sbin/modprobe i2c_hid\n  ;;\nesac\n\n\n\nAnd now the touchpad is as happy post-suspend as it is on a fresh restart. Happy days!\n",
      tags: ["technology","hp-probook","touchpad"],
      id: 4
    });
    

    index.add({
      title: "Cisco Meraki Client VPN on Ubuntu 19.04/19.10/20.04",
      category: ["technology"],
      content: "\n    \n    Photo of Meraki MX84 Meraki Firewall from Cisco Meraki website\n\n\nWith my beloved, and worn, day to day laptop having to go in for repair, I had to setup a temporary laptop to work on for a few weeks.\n\nAt work we use Cisco Meraki devices in many places, including the edge of network for our various offices. Whilst their main use is to form a mesh network around our offices and server infrastructure, we also use them to enable a lightweight Client VPN solution.\n\nThe Cisco Meraki Client VPN option provides a L2TP/IPsec based VPN using either its own internal user store, an LDAP Directory, Microsoft Active Directory, or a Radius server to authenticate users.\n\nCisco Meraki provide great instructions for Windows, Mac and mobile devices, but really old instructions for Linux. Therefore, I am posting this as much to remind me the next time I need to set it up as to help others.\n\n\n\nInstall L2TP Plugins for Network Manager\n\nBy default, support for L2TP VPNs is not installed for Network Manager, so we need to install them:\n\nsudo apt-get install network-manager-l2tp\nsudo apt-get install network-manager-l2tp-gnome\n\n\n\nDisable System xl2tpd\n\nNetwork Manager spawns and manages its own instance of xl2tpd so if there is a system instance still running it will not be able to use UDP port 1701, and will instead use an ephemeral port (i.e. random high port).\n\nTo stop this from happening, we need to stop the deamon and disabling it from starting again:\n\nsudo systemctl stop xl2tpd\nsudo systemctl disable xl2tpd\n\n\nSetup Your VPN Connection\n\nNow you are ready to add your VPN connection. Having taken the steps above, we‚Äôve Gnome Network Manager settings panel now includes the option to add L2TP VPN connections:\n\n\n\nThe main settings we need to customise to work with Cisco Meraki Client VPN are on the Identity tab.\n\n\n\nWe can give our VPN a name, set the VPN gateway, and add our user credentials (with optional NT Domain depending whether Active Directory is used as the authentication scheme).\n\nWe now need to set our IPsec and PPP settings.\n\nIPsec Settings\n\n\n\nIn the IPsec Settings we need to tick the Enable IPsec tunnel to L2TP host checkbox , expand the Advanced settings, and then add three things:\n\n\n  Pre-shared Key - adding the key provided by your network administrator\n  Phase1 Algorithm - use the following 3des-sha1-modp1024\n  Phase2 Algorithm - use the following 3des-sha1\n\n\nClick OK to set this on the connection.\n\nPPP Settings\n\n\n\nIn the PPP Settings we need to make sure PPP is the only Authentication mechanism selected.\n\nThe other defaults should be OK, however I‚Äôve included a screenshot to confirm against above.\n\nClick OK to set this on the connection.\n\nUse Your VPN Connection\n\nThe VPN should now be available in the Gnome Settings panel:\n\n\n\nand in the main Gnome Menu for quick connect/disconnect\n\n\n",
      tags: ["technology","meraki","vpn"],
      id: 5
    });
    

    index.add({
      title: "Hello world",
      category: ["personal"],
      content: "\n    \n\n\nHello world! Please bear with me, I am just getting setup.\n",
      tags: ["personal"],
      id: 6
    });
    


var store = [{
    "title": "Development SSL for .NET Core and NGINX in Docker",
    "link": "/opensource/development-ssl-dotnetcore-docker.html",
    "image": null,
    "date": "August 3, 2022",
    "category": ["opensource"],
    "excerpt": "Photo by James Sutton on Unsplash We‚Äôve all been there. You want to use SSL in development to mirror a..."
},{
    "title": "Okapi Maven Plugin",
    "link": "/localization/opensource/new-okapi-maven-plugin.html",
    "image": null,
    "date": "July 16, 2022",
    "category": ["localization","opensource"],
    "excerpt": "Photo by Safar Safarov on Unsplash In the work we did at Lingo24 when I was there, we made use..."
},{
    "title": "Apache Tika Docker Examples",
    "link": "/technology/opensource/apache-tika-docker-examples.html",
    "image": null,
    "date": "December 28, 2020",
    "category": ["technology","opensource"],
    "excerpt": "Photo by Priscilla Du Preez on Unsplash For a number of years I‚Äôve been involved in the Apache Tika project..."
},{
    "title": "Apache Tika and the ObjectRecognitionParser for Object Recognition and Captioning Using TensorFlow REST.",
    "link": "/technology/opensource/apache-tika-and-objectrecognitionparser-tensorflow-rest.html",
    "image": null,
    "date": "December 1, 2020",
    "category": ["technology","opensource"],
    "excerpt": "Photo by John Schnobrich on Unsplash One of the coolest new features added to Apache Tika in the past few..."
},{
    "title": "HP Probook Touchpad Slow After Supend Ubuntu 19.04",
    "link": "/technology/hp-probook-touchpad-slow-after-suspend-ubuntu.html",
    "image": null,
    "date": "December 6, 2019",
    "category": ["technology"],
    "excerpt": "Photo of a Laptop Touchpad Having got used to my replacement laptop, I‚Äôve decided to keep using it, but there..."
},{
    "title": "Cisco Meraki Client VPN on Ubuntu 19.04/19.10/20.04",
    "link": "/technology/cisco-meraki-client-vpn-on-ubuntu-1904.html",
    "image": null,
    "date": "August 2, 2019",
    "category": ["technology"],
    "excerpt": "Photo of Meraki MX84 Meraki Firewall from Cisco Meraki website With my beloved, and worn, day to day laptop having..."
},{
    "title": "Hello world",
    "link": "/personal/helloworld.html",
    "image": null,
    "date": "January 6, 2019",
    "category": ["personal"],
    "excerpt": "\n    \n\n\nHello world! Please bear with me, I am just getting setup.\n"
}]

$(document).ready(function() {
    $('#search-input').on('keyup', function () {
        var resultdiv = $('#results-container');
        if (!resultdiv.is(':visible'))
            resultdiv.show();
        var query = $(this).val();
        var result = index.search(query);
        resultdiv.empty();
        $('.show-results-count').text(result.length + ' Results');
        for (var item in result) {
            var ref = result[item].ref;
            var searchitem = '<li><a href="'+ hostname + store[ref].link+'">'+store[ref].title+'</a></li>';
            resultdiv.append(searchitem);
        }
    });
});